{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeetCode SQL Problems\n",
    "\n",
    "### Table of Contents\n",
    "- [Problem 1757: Recyclable and Low Fat Products](#Problem-1757-Recyclable-and-Low-Fat-Products) EASY, SELECT\n",
    "- [Problem 584: Find Customer Referee](#Problem-584-Find-Customer-Referee) EASY, SELECT, HandleNull\n",
    "- [Problem 595: Big Countries](#Problem-595-Big-Countries) EASY, SELECT\n",
    "- [Problem 1148: Article Views I](#Problem-1148-Article-Views-I) EASY, SELECT\n",
    "- [Problem 1683: Invalid Tweets](#problem-1683-nvalid-tweets) EASY,SELECT\n",
    "-  Problem 1378. Replace Employee ID With The Unique Identifier.  EASY, JOIN\n",
    "-  Problem 1068. Product Sales Analysis I. EASY, JOIN\n",
    "-  [1581. Customer Who Visited but Did Not Make Any Transactions](https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/?envType=study-plan-v2&envId=top-sql-50) EASY*, JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window as W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('LeetCode Problems') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-14 21:02:39--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.166.97.40, 3.166.97.18, 3.166.97.164, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.166.97.40|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 61170186 (58M) [binary/octet-stream]\n",
      "Saving to: ‘data/yellow_taxi/yellow_tripdata_2024-09.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  58.34M  8.05MB/s    in 7.7s    \n",
      "\n",
      "2024-12-14 21:02:47 (7.62 MB/s) - ‘data/yellow_taxi/yellow_tripdata_2024-09.parquet’ saved [61170186/61170186]\n",
      "\n",
      "--2024-12-14 21:02:47--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.166.97.40, 3.166.97.18, 3.166.97.164, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.166.97.40|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘data/yellow_taxi/taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-14 21:02:47 (32.8 MB/s) - ‘data/yellow_taxi/taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download Taxi trip yellow data and zones\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet -P data/yellow_taxi/\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv -P data/yellow_taxi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Yellow Taxi Trip Data (Parquet)\n",
    "yellow_taxi_df = spark.read.parquet(\"data/yellow_taxi/yellow_tripdata_2024-09.parquet\")\n",
    "\n",
    "# Load the Taxi Zone Lookup Data (CSV)\n",
    "taxi_zone_df = spark.read.csv(\"data/yellow_taxi/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1757: Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```md\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| product_id  | int     |\n",
    "| low_fats    | enum    |\n",
    "| recyclable  | enum    |\n",
    "+-------------+---------+\n",
    "product_id is the primary key (column with unique values) for this table.\n",
    "low_fats is an ENUM (category) of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.\n",
    "recyclable is an ENUM (category) of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.\n",
    "\n",
    " \n",
    "\n",
    "Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Products table:\n",
    "+-------------+----------+------------+\n",
    "| product_id  | low_fats | recyclable |\n",
    "+-------------+----------+------------+\n",
    "| 0           | Y        | N          |\n",
    "| 1           | Y        | Y          |\n",
    "| 2           | N        | Y          |\n",
    "| 3           | Y        | Y          |\n",
    "| 4           | N        | N          |\n",
    "+-------------+----------+------------+\n",
    "Output: \n",
    "+-------------+\n",
    "| product_id  |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 3           |\n",
    "+-------------+\n",
    "Explanation: Only products 1 and 3 are both low fat and recyclable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS SQL Server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```sql\n",
    "SELECT product_id\n",
    "From Products \n",
    "WHERE low_fats = 'Y' and recyclable = 'Y';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (0, 'Y', 'N'),\n",
    "    (1, 'Y', 'Y'),\n",
    "    (2, 'N', 'Y'),\n",
    "    (3, 'Y', 'Y'),\n",
    "    (4, 'N', 'N'),\n",
    "    (5, 'Y', 'N'),\n",
    "    (6, 'N', 'Y'),\n",
    "    (7, 'Y', 'Y'),\n",
    "    (8, 'N', 'N'),\n",
    "    (9, 'Y', 'Y'),\n",
    "    (10, 'N', 'N')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "columns = ['product_id', 'low_fats', 'recyclable']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+-------------+\n",
    "| product_id  |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 3           |\n",
    "| 7           |\n",
    "| 9           |\n",
    "+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter((F.col('low_fats') == 'Y') & (F.col('recyclable') == 'Y'))\n",
    "    .select('product_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         7|\n",
      "|         9|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 584: Find Customer Referee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Table: Customer\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| referee_id  | int     |\n",
    "+-------------+---------+\n",
    "In SQL, id is the primary key column for this table.\n",
    "Each row of this table indicates the id of a customer, their name, and the id of the customer who referred them.\n",
    "\n",
    " \n",
    "\n",
    "Find the names of the customer that are not referred by the customer with id = 2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "+----+------+------------+\n",
    "| id | name | referee_id |\n",
    "+----+------+------------+\n",
    "| 1  | Will | null       |\n",
    "| 2  | Jane | null       |\n",
    "| 3  | Alex | 2          |\n",
    "| 4  | Bill | null       |\n",
    "| 5  | Zack | 1          |\n",
    "| 6  | Mark | 2          |\n",
    "+----+------+------------+\n",
    "Output: \n",
    "+------+\n",
    "| name |\n",
    "+------+\n",
    "| Will |\n",
    "| Jane |\n",
    "| Bill |\n",
    "| Zack |\n",
    "+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT [name]\n",
    "FROM Customer\n",
    "where COALESCE(referee_id,0) != 2;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  name|referee_id|\n",
      "+---+------+----------+\n",
      "|  1|  Will|      NULL|\n",
      "|  2|  Jane|      NULL|\n",
      "|  3|  Alex|         2|\n",
      "|  4|  Bill|      NULL|\n",
      "|  5|  Zack|         1|\n",
      "|  6|  Mark|         2|\n",
      "|  7|  Emma|         3|\n",
      "|  8|  John|      NULL|\n",
      "|  9|  Lucy|         4|\n",
      "| 10| David|         2|\n",
      "| 11|Sophie|         5|\n",
      "| 12|  Adam|      NULL|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2),\n",
    "    (7, \"Emma\", 3),\n",
    "    (8, \"John\", None),\n",
    "    (9, \"Lucy\", 4),\n",
    "    (10, \"David\", 2),\n",
    "    (11, \"Sophie\", 5),\n",
    "    (12, \"Adam\", None)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"referee_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter((F.col('referee_id') != 2) | (F.col(\"referee_id\").isNull()))\n",
    "    .select('name')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|  Will|\n",
      "|  Jane|\n",
      "|  Bill|\n",
      "|  Zack|\n",
      "|  Emma|\n",
      "|  John|\n",
      "|  Lucy|\n",
      "|Sophie|\n",
      "|  Adam|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 595: Big Countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Table: World\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| name        | varchar |\n",
    "| continent   | varchar |\n",
    "| area        | int     |\n",
    "| population  | int     |\n",
    "| gdp         | bigint  |\n",
    "+-------------+---------+\n",
    "name is the primary key (column with unique values) for this table.\n",
    "Each row of this table gives information about the name of a country, the continent to which it belongs, its area, the population, and its GDP value.\n",
    "\n",
    " \n",
    "\n",
    "A country is big if:\n",
    "\n",
    "    it has an area of at least three million (i.e., 3000000 km2), or\n",
    "    it has a population of at least twenty-five million (i.e., 25000000).\n",
    "\n",
    "Write a solution to find the name, population, and area of the big countries.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "World table:\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "| name        | continent | area    | population | gdp          |\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "| Afghanistan | Asia      | 652230  | 25500100   | 20343000000  |\n",
    "| Albania     | Europe    | 28748   | 2831741    | 12960000000  |\n",
    "| Algeria     | Africa    | 2381741 | 37100000   | 188681000000 |\n",
    "| Andorra     | Europe    | 468     | 78115      | 3712000000   |\n",
    "| Angola      | Africa    | 1246700 | 20609294   | 100990000000 |\n",
    "+-------------+-----------+---------+------------+--------------+\n",
    "Output: \n",
    "+-------------+------------+---------+\n",
    "| name        | population | area    |\n",
    "+-------------+------------+---------+\n",
    "| Afghanistan | 25500100   | 652230  |\n",
    "| Algeria     | 37100000   | 2381741 |\n",
    "+-------------+------------+---------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT [name], [population], [area]\n",
    "from World\n",
    "where [area] >= 3000000 OR [population] >= 25000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"continent\", \"area\", \"population\", \"gdp\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter((F.col('population') > 25000000) | (F.col('area') > 3000000))\n",
    "    .select('name','population', 'area')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|       name|population|   area|\n",
      "+-----------+----------+-------+\n",
      "|Afghanistan|  25500100| 652230|\n",
      "|    Algeria|  37100000|2381741|\n",
      "+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1148: Article Views I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Table: Views\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| article_id    | int     |\n",
    "| author_id     | int     |\n",
    "| viewer_id     | int     |\n",
    "| view_date     | date    |\n",
    "+---------------+---------+\n",
    "There is no primary key (column with unique values) for this table, the table may have duplicate rows.\n",
    "Each row of this table indicates that some viewer viewed an article (written by some author) on some date. \n",
    "Note that equal author_id and viewer_id indicate the same person.\n",
    "\n",
    " \n",
    "\n",
    "Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "\n",
    "Return the result table sorted by id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Views table:\n",
    "+------------+-----------+-----------+------------+\n",
    "| article_id | author_id | viewer_id | view_date  |\n",
    "+------------+-----------+-----------+------------+\n",
    "| 1          | 3         | 5         | 2019-08-01 |\n",
    "| 1          | 3         | 6         | 2019-08-02 |\n",
    "| 2          | 7         | 7         | 2019-08-01 |\n",
    "| 2          | 7         | 6         | 2019-08-02 |\n",
    "| 4          | 7         | 1         | 2019-07-22 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "| 3          | 4         | 4         | 2019-07-21 |\n",
    "+------------+-----------+-----------+------------+\n",
    "Output: \n",
    "+------+\n",
    "| id   |\n",
    "+------+\n",
    "| 4    |\n",
    "| 7    |\n",
    "+------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "select DISTINCT author_id as id\n",
    "from Views\n",
    "where author_id = viewer_id \n",
    "ORDER BY author_id;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 3, 5, '2019-08-01'),\n",
    "    (1, 3, 6, '2019-08-02'),\n",
    "    (2, 7, 7, '2019-08-01'),\n",
    "    (2, 7, 6, '2019-08-02'),\n",
    "    (4, 7, 1, '2019-07-22'),\n",
    "    (3, 4, 4, '2019-07-21'),\n",
    "    (3, 4, 4, '2019-07-21')\n",
    "]\n",
    "\n",
    "columns = [\"article_id\", \"author_id\", \"viewer_id\", \"view_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter(F.col('author_id') == F.col('viewer_id'))\n",
    "    .select('author_id')\n",
    "    .distinct()\n",
    "    .orderBy('author_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|author_id|\n",
      "+---------+\n",
      "|        4|\n",
      "|        7|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1683: Invalid Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| tweet_id       | int     |\n",
    "| content        | varchar |\n",
    "+----------------+---------+\n",
    "tweet_id is the primary key (column with unique values) for this table.\n",
    "content consists of characters on an American Keyboard, and no other special characters.\n",
    "This table contains all the tweets in a social media app.\n",
    "\n",
    " \n",
    "\n",
    "Write a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Tweets table:\n",
    "+----------+-----------------------------------+\n",
    "| tweet_id | content                           |\n",
    "+----------+-----------------------------------+\n",
    "| 1        | Let us Code                       |\n",
    "| 2        | More than fifteen chars are here! |\n",
    "+----------+-----------------------------------+\n",
    "Output: \n",
    "+----------+\n",
    "| tweet_id |\n",
    "+----------+\n",
    "| 2        |\n",
    "+----------+\n",
    "Explanation: \n",
    "Tweet 1 has length = 11. It is a valid tweet.\n",
    "Tweet 2 has length = 33. It is an invalid tweet.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT tweet_id\n",
    "from Tweets\n",
    "where LEN(content) > 15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 'Let us Code'),\n",
    "    (2, 'More than fifteen chars are here!')\n",
    "]\n",
    "\n",
    "columns = [\"tweet_id\", \"content\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter(F.length(F.col(\"content\")) > 15)\n",
    "    .select(\"tweet_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1378. Replace Employee ID With The Unique Identifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Table: Employees\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the id and the name of an employee in a company.\n",
    "\n",
    " \n",
    "\n",
    "Table: EmployeeUNI\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| unique_id     | int     |\n",
    "+---------------+---------+\n",
    "(id, unique_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table contains the id and the corresponding unique id of an employee in the company.\n",
    "\n",
    " \n",
    "\n",
    "Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+----+----------+\n",
    "| id | name     |\n",
    "+----+----------+\n",
    "| 1  | Alice    |\n",
    "| 7  | Bob      |\n",
    "| 11 | Meir     |\n",
    "| 90 | Winston  |\n",
    "| 3  | Jonathan |\n",
    "+----+----------+\n",
    "EmployeeUNI table:\n",
    "+----+-----------+\n",
    "| id | unique_id |\n",
    "+----+-----------+\n",
    "| 3  | 1         |\n",
    "| 11 | 2         |\n",
    "| 90 | 3         |\n",
    "+----+-----------+\n",
    "Output: \n",
    "+-----------+----------+\n",
    "| unique_id | name     |\n",
    "+-----------+----------+\n",
    "| null      | Alice    |\n",
    "| null      | Bob      |\n",
    "| 2         | Meir     |\n",
    "| 3         | Winston  |\n",
    "| 1         | Jonathan |\n",
    "+-----------+----------+\n",
    "Explanation: \n",
    "Alice and Bob do not have a unique ID, We will show null instead.\n",
    "The unique ID of Meir is 2.\n",
    "The unique ID of Winston is 3.\n",
    "The unique ID of Jonathan is 1.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "select EUI.unique_id, EMP.[name]\n",
    "from Employees as EMP\n",
    "left join EmployeeUNI as EUI on EMP.id = EUI.id; \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_data = [(1, 'Alice'),\n",
    "                  (7, 'Bob'),\n",
    "                  (11, 'Meir'),\n",
    "                  (90, 'Winston'),\n",
    "                  (3, 'Jonathan')]\n",
    "employeeuni_data = [(3, 1),\n",
    "                    (11, 2),\n",
    "                    (90, 3)]\n",
    "\n",
    "\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, ['id', 'name'])\n",
    "employeeuni_df = spark.createDataFrame(employeeuni_data, ['id', 'unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = employees_df.join(employeeuni_df, on='id', how='left') \\\n",
    "                        .select('unique_id', 'name') \\\n",
    "                        .orderBy('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     NULL|   Alice|\n",
      "|     NULL|     Bob|\n",
      "|        1|Jonathan|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If `employees_df` is significantly larger than `employeeuni_df`, then broadcast `employeeuni_df` to all worker nodes to minimize the costly shuffle operation and optimize the join performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pyspark.sql.functions.broadcast(df)[source]\n",
    "\n",
    "    Marks a DataFrame as small enough for use in broadcast joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast the smaller DataFrame (employeeuni_df)\n",
    "result = employees_df.join(F.broadcast(employeeuni_df), on='id', how='left') \\\n",
    "                        .select('unique_id', 'name') \\\n",
    "                        .orderBy('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     NULL|   Alice|\n",
      "|     NULL|     Bob|\n",
      "|        1|Jonathan|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_df.printSchema()\n",
    "taxi_zone_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = yellow_taxi_df.join(taxi_zone_df, yellow_taxi_df.PULocationID == taxi_zone_df.LocationID, \"left\") \\\n",
    "    .select(\"PULocationID\", \"DOLocationID\", \"Trip_distance\", \"Total_amount\", \"Zone\",\"Borough\") \\\n",
    "    .orderBy(\"tpep_pickup_datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>Trip_distance</th>\n",
       "      <th>Total_amount</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163</td>\n",
       "      <td>237</td>\n",
       "      <td>0.71</td>\n",
       "      <td>12.20</td>\n",
       "      <td>Midtown North</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>100</td>\n",
       "      <td>17.86</td>\n",
       "      <td>105.23</td>\n",
       "      <td>JFK Airport</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>230</td>\n",
       "      <td>2.05</td>\n",
       "      <td>33.25</td>\n",
       "      <td>East Chelsea</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>235</td>\n",
       "      <td>7.77</td>\n",
       "      <td>47.40</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>237</td>\n",
       "      <td>3.58</td>\n",
       "      <td>32.28</td>\n",
       "      <td>Morningside Heights</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PULocationID  DOLocationID  Trip_distance  Total_amount  \\\n",
       "0           163           237           0.71         12.20   \n",
       "1           132           100          17.86        105.23   \n",
       "2            68           230           2.05         33.25   \n",
       "3           141           235           7.77         47.40   \n",
       "4           166           237           3.58         32.28   \n",
       "\n",
       "                  Zone    Borough  \n",
       "0        Midtown North  Manhattan  \n",
       "1          JFK Airport     Queens  \n",
       "2         East Chelsea  Manhattan  \n",
       "3      Lenox Hill West  Manhattan  \n",
       "4  Morningside Heights  Manhattan  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1068. Product Sales Analysis I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Table: Sales\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "+-------------+-------+\n",
    "(sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "product_id is a foreign key (reference column) to Product table.\n",
    "Each row of this table shows a sale on the product product_id in a certain year.\n",
    "Note that the price is per unit.\n",
    "\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "+--------------+---------+\n",
    "product_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table indicates the product name of each product.\n",
    "\n",
    " \n",
    "\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+---------+------------+------+----------+-------+\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "+---------+------------+------+----------+-------+ \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "+---------+------------+------+----------+-------+\n",
    "Product table:\n",
    "+------------+--------------+\n",
    "| product_id | product_name |\n",
    "+------------+--------------+\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+--------------+-------+-------+\n",
    "| product_name | year  | price |\n",
    "+--------------+-------+-------+\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "+--------------+-------+-------+\n",
    "Explanation: \n",
    "From sale_id = 1, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "From sale_id = 2, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "From sale_id = 7, we can conclude that Apple was sold for 9000 in the year 2011.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "Select P.product_name, S.[year], S.price\n",
    "from Product as P\n",
    "inner join Sales as S \n",
    "on S.product_id = P.product_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"SalesProductJoin\").getOrCreate()\n",
    "\n",
    "\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
    "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
    "\n",
    "\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "product_columns = [\"product_id\", \"product_name\"]\n",
    "product_df = spark.createDataFrame(product_data, product_columns)\n",
    "\n",
    "\n",
    "result = sales_df.join(product_df, on=\"product_id\", how=\"inner\") \\\n",
    "                    .select(\"product_name\", \"year\", \"price\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1581. Customer Who Visited but Did Not Make Any Transactions](https://leetcode.com/problems/customer-who-visited-but-did-not-make-any-transactions/?envType=study-plan-v2&envId=top-sql-50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "select V.customer_id, Count(V.visit_id) as count_no_trans\n",
    "from Visits as V \n",
    "left join Transactions as T\n",
    "on V.visit_id = T.visit_id\n",
    "WHERE T.transaction_id IS NULL \n",
    "GROUP BY V.customer_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits_data = [\n",
    "    (1, 23),\n",
    "    (2, 9),\n",
    "    (4, 30),\n",
    "    (5, 54),\n",
    "    (6, 96),\n",
    "    (7, 54),\n",
    "    (8, 54)\n",
    "]\n",
    "visits_columns = [\"visit_id\", \"customer_id\"]\n",
    "visits_df = spark.createDataFrame(visits_data, visits_columns)\n",
    "\n",
    "transactions_data = [\n",
    "    (2, 5, 310),\n",
    "    (3, 5, 300),\n",
    "    (9, 5, 200),\n",
    "    (12, 1, 910),\n",
    "    (13, 2, 970)\n",
    "]\n",
    "transactions_columns = [\"transaction_id\", \"visit_id\", \"amount\"]\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    visits_df\n",
    "    .join(transactions_df, on=\"visit_id\", how=\"left\")\n",
    "    .where(transactions_df.transaction_id.isNull())\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(F.count(\"visit_id\").alias(\"count_no_trans\"))\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[197. Rising Temperature](https://leetcode.com/problems/rising-temperature/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH WeatherWithPrevious AS (\n",
    "    SELECT \n",
    "        id,\n",
    "        recordDate,\n",
    "        temperature,\n",
    "        LAG(recordDate) OVER (ORDER BY recordDate) as prev_date,\n",
    "        LAG(temperature) OVER (ORDER BY recordDate) as prev_temp\n",
    "    FROM Weather\n",
    ")\n",
    "SELECT id\n",
    "FROM WeatherWithPrevious\n",
    "WHERE temperature > prev_temp\n",
    "AND DATEDIFF(day, prev_date, recordDate) = 1;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark\n",
    "\n",
    "data = [\n",
    "    (1, \"2015-01-01\", 10),\n",
    "    (2, \"2015-01-02\", 25),\n",
    "    (3, \"2015-01-03\", 20),\n",
    "    (4, \"2015-01-04\", 30),\n",
    "    (5, \"2015-01-06\", 40) \n",
    "]\n",
    "columns = [\"id\", \"recordDate\", \"temperature\"]\n",
    "\n",
    "weather_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "window_spec = Window.orderBy(\"recordDate\")\n",
    "\n",
    "result_df = (\n",
    "    weather_df\n",
    "    .withColumn(\"prev_date\", \n",
    "                F.lag(\"recordDate\").over(window_spec))\n",
    "    .withColumn(\"prev_temp\", \n",
    "                F.lag(\"temperature\").over(window_spec))\n",
    "    .withColumn(\"days_diff\", \n",
    "                F.datediff(\"recordDate\", \"prev_date\"))\n",
    "    .filter(\n",
    "        (F.col(\"temperature\") > F.col(\"prev_temp\")) & \n",
    "        (F.col(\"days_diff\") == 1)\n",
    "    )\n",
    "    .select(\"id\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Problem 1661. Average Time of Process per Machine](https://leetcode.com/problems/average-time-of-process-per-machine/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH ProcessTimes AS (\n",
    "    SELECT \n",
    "        machine_id,\n",
    "        process_id,\n",
    "        MAX(CASE WHEN activity_type = 'end' THEN timestamp END) - \n",
    "        MAX(CASE WHEN activity_type = 'start' THEN timestamp END) as process_time\n",
    "    FROM Activity\n",
    "    GROUP BY machine_id, process_id\n",
    ")\n",
    "SELECT \n",
    "    machine_id,\n",
    "    ROUND(AVG(process_time), 3) as processing_time\n",
    "FROM ProcessTimes\n",
    "GROUP BY machine_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ProcessTimeAnalysis\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (0, 0, \"start\", 0.712),\n",
    "    (0, 0, \"end\", 1.520),\n",
    "    (0, 1, \"start\", 3.140),\n",
    "    (0, 1, \"end\", 4.120),\n",
    "    (1, 0, \"start\", 0.550),\n",
    "    (1, 0, \"end\", 1.550),\n",
    "    (1, 1, \"start\", 0.430),\n",
    "    (1, 1, \"end\", 1.420),\n",
    "    (2, 0, \"start\", 4.100),\n",
    "    (2, 0, \"end\", 4.512),\n",
    "    (2, 1, \"start\", 2.500),\n",
    "    (2, 1, \"end\", 5.000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"machine_id\", IntegerType(), False),\n",
    "    StructField(\"process_id\", IntegerType(), False),\n",
    "    StructField(\"activity_type\", StringType(), False),\n",
    "    StructField(\"timestamp\", FloatType(), False)\n",
    "])\n",
    "\n",
    "activity_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = (\n",
    "    activity_df\n",
    "    .groupBy(\"machine_id\", \"process_id\")\n",
    "    .agg(\n",
    "        (F.max(F.when(F.col(\"activity_type\") == \"end\", F.col(\"timestamp\")).otherwise(None)) - \n",
    "        F.max(F.when(F.col(\"activity_type\") == \"start\", F.col(\"timestamp\")).otherwise(None))).alias(\"process_time\")\n",
    "    )\n",
    "    .groupBy(\"machine_id\")\n",
    "    .agg(\n",
    "        F.round(F.avg(\"process_time\"), 3).alias(\"processing_time\")\n",
    "    ).orderBy(\"machine_id\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[577. Employee Bonus](https://leetcode.com/problems/employee-bonus/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "select [name], bonus\n",
    "from Employee as E\n",
    "left join Bonus as B on E.empId = B.empId\n",
    "where Coalesce(bonus,0) < 1000 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Employee Bonus Filter\").getOrCreate()\n",
    "\n",
    "# Create Employee DataFrame\n",
    "data_employee = [\n",
    "    (3, \"Brad\", None, 4000),\n",
    "    (1, \"John\", 3, 1000),\n",
    "    (2, \"Dan\", 3, 2000),\n",
    "    (4, \"Thomas\", 3, 4000)\n",
    "]\n",
    "columns_employee = [\"empId\", \"name\", \"supervisor\", \"salary\"]\n",
    "employee_df = spark.createDataFrame(data_employee, columns_employee)\n",
    "\n",
    "# Create Bonus DataFrame\n",
    "data_bonus = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "columns_bonus = [\"empId\", \"bonus\"]\n",
    "bonus_df = spark.createDataFrame(data_bonus, columns_bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = (\n",
    "    employee_df\n",
    "    .join(bonus_df, on=\"empId\",how=\"left\")\n",
    "    .filter((F.col(\"bonus\").isNull()) | (F.col(\"bonus\") < 1000))\n",
    "    .select(\"name\", \"bonus\")\n",
    "    \n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1280. Students and Examinations](https://leetcode.com/problems/students-and-examinations/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    S.student_id\n",
    "    ,S.student_name\n",
    "    ,SU.subject_name\n",
    "    ,COUNT(E.student_id) attended_exams\n",
    "FROM Students S\n",
    "CROSS JOIN Subjects SU\n",
    "LEFT JOIN Examinations E\n",
    "    ON E.student_id = S.student_id\n",
    "    AND SU.subject_name = E.subject_name\n",
    "GROUP BY S.student_id, S.student_name, SU.subject_name\n",
    "ORDER BY S.student_id, SU.subject_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (13, \"John\"),\n",
    "    (6, \"Alex\")\n",
    "]\n",
    "students_columns = [\"student_id\", \"student_name\"]\n",
    "students_df = spark.createDataFrame(students_data, students_columns)\n",
    "\n",
    "# Sample data for Subjects table\n",
    "subjects_data = [\n",
    "    (\"Math\",),\n",
    "    (\"Physics\",),\n",
    "    (\"Programming\",)\n",
    "]\n",
    "subjects_columns = [\"subject_name\"]\n",
    "subjects_df = spark.createDataFrame(subjects_data, subjects_columns)\n",
    "\n",
    "# Sample data for Examinations table\n",
    "examinations_data = [\n",
    "    (1, \"Math\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Programming\"),\n",
    "    (2, \"Programming\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Math\"),\n",
    "    (13, \"Math\"),\n",
    "    (13, \"Programming\"),\n",
    "    (13, \"Physics\"),\n",
    "    (2, \"Math\"),\n",
    "    (1, \"Math\")\n",
    "]\n",
    "examinations_columns = [\"student_id\", \"subject_name\"]\n",
    "examinations_df = spark.createDataFrame(examinations_data, examinations_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = students_df.crossJoin(subjects_df)\n",
    "print(\"Base Dataframe\",base_df.show())\n",
    "# Count examinations for each student and subject\n",
    "exam_counts = (\n",
    "    examinations_df\n",
    "    .groupBy(\"student_id\", \"subject_name\")\n",
    "    .agg(F.count(\"*\").alias(\"attended_exams\"))\n",
    ")\n",
    "print(\"Exam Counts\",exam_counts.show())\n",
    "# Join the base table with exam counts and fill nulls with 0\n",
    "result_df = (\n",
    "    base_df\n",
    "    .join(exam_counts, \n",
    "          [\"student_id\", \"subject_name\"], \n",
    "          \"left\")\n",
    "    .fillna(0, subset=[\"attended_exams\"])\n",
    "    .orderBy(\"student_id\", \"subject_name\")\n",
    ")\n",
    "print(\"Result Dataframe\",result_df.show())\n",
    "# Select and order columns to match expected output\n",
    "final_df = (\n",
    "    result_df\n",
    "    .select(\n",
    "        \"student_id\",\n",
    "        \"student_name\",\n",
    "        \"subject_name\",\n",
    "        F.col(\"attended_exams\")\n",
    "    )\n",
    ")\n",
    "print(\"Final Dataframe\",final_df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[570. Managers with at Least 5 Direct Reports](https://leetcode.com/problems/managers-with-at-least-5-direct-reports/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT e1.*, e2.*\n",
    "FROM Employee e1\n",
    "JOIN Employee e2\n",
    "ON e1.id = e2.managerId\n",
    "GROUP BY e1.id, e1.name\n",
    "HAVING COUNT(e1.id) >= 5;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"John\", \"A\", None),\n",
    "    (102, \"Dan\", \"A\", 101),\n",
    "    (103, \"James\", \"A\", 101),\n",
    "    (104, \"Amy\", \"A\", 101),\n",
    "    (105, \"Anne\", \"A\", 101),\n",
    "    (106, \"Ron\", \"B\", 101)\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"name\", \"department\", \"managerId\"]\n",
    "employee_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "result_df = employee_df.alias(\"e1\") \\\n",
    "        .join(employee_df.alias(\"e2\"), \n",
    "              employee_df.id == employee_df.alias(\"e2\").managerId) \\\n",
    "        .groupBy(\"e1.id\", \"e1.name\") \\\n",
    "        .agg(F.count(\"e2.id\").alias(\"direct_reports\")) \\\n",
    "        .filter(\"direct_reports >= 5\") \\\n",
    "        .select(\"name\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1934. Confirmation Rate](https://leetcode.com/problems/confirmation-rate/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \n",
    "    S.user_id, \n",
    "    ROUND(\n",
    "        AVG(\n",
    "            CASE WHEN c.action = 'confirmed' THEN 1.00 ELSE 0.00 END\n",
    "            )\n",
    "        ,2) AS confirmation_rate\n",
    "FROM \n",
    "    Signups AS S\n",
    "LEFT JOIN Confirmations as C on S.user_id = C.user_id\n",
    "GROUP BY \n",
    "    S.user_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signups_data = [\n",
    "    (3, \"2020-03-21 10:16:13\"),\n",
    "    (7, \"2020-01-04 13:57:59\"),\n",
    "    (2, \"2020-07-29 23:09:44\"),\n",
    "    (6, \"2020-12-09 10:39:37\"),\n",
    "]\n",
    "confirmations_data = [\n",
    "    (3, \"2021-01-06 03:30:46\", \"timeout\"),\n",
    "    (3, \"2021-07-14 14:00:00\", \"timeout\"),\n",
    "    (7, \"2021-06-12 11:57:29\", \"confirmed\"),\n",
    "    (7, \"2021-06-13 12:58:28\", \"confirmed\"),\n",
    "    (7, \"2021-06-14 13:59:27\", \"confirmed\"),\n",
    "    (2, \"2021-01-22 00:00:00\", \"confirmed\"),\n",
    "    (2, \"2021-02-28 23:59:59\", \"timeout\"),\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "signups_df = spark.createDataFrame(signups_data, [\"user_id\", \"time_stamp\"])\n",
    "confirmations_df = spark.createDataFrame(confirmations_data, [\"user_id\", \"time_stamp\", \"action\"])\n",
    "\n",
    "result = (\n",
    "    signups_df\n",
    "    .join(confirmations_df, on=\"user_id\", how=\"left\")\n",
    "    .withColumn(\"is_confirmed\", F.when(F.col(\"action\") == \"confirmed\", 1.0).otherwise(0.0))\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(F.round(F.avg(\"is_confirmed\"), 2).alias(\"confirmation_rate\"))\n",
    "    .fillna({\"confirmation_rate\": 0.0})  # Users with no requests have a rate of 0\n",
    ")\n",
    "# Show result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1174. Immediate Food Delivery II](https://leetcode.com/problems/immediate-food-delivery-ii/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH RankedDelivery AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        customer_pref_delivery_date,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) AS RowRank\n",
    "    FROM \n",
    "        Delivery\n",
    ")\n",
    "SELECT \n",
    "    ROUND(AVG(CASE WHEN customer_pref_delivery_date = order_date THEN 1.0 ELSE 0.0 END) * 100, 2) AS immediate_percentage\n",
    "FROM  \n",
    "    RankedDelivery\n",
    "WHERE \n",
    "    RowRank = 1;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[550. Game Play Analysis IV](https://leetcode.com/problems/game-play-analysis-iv/description/?envType=study-plan-v2&envId=top-sql-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH FirstLogins AS (\n",
    "    SELECT \n",
    "        player_id,\n",
    "        MIN(event_date) as first_login_date\n",
    "    FROM Activity\n",
    "    GROUP BY player_id\n",
    "),\n",
    "ConsecutiveLogins AS (\n",
    "    SELECT \n",
    "        f.player_id,\n",
    "        CASE \n",
    "            WHEN EXISTS (\n",
    "                SELECT 1 \n",
    "                FROM Activity a \n",
    "                WHERE a.player_id = f.player_id \n",
    "                AND a.event_date = DATEADD(day, 1, f.first_login_date)\n",
    "            ) THEN 1 \n",
    "            ELSE 0 \n",
    "        END as logged_consecutive\n",
    "    FROM FirstLogins f\n",
    ")\n",
    "SELECT \n",
    "    ROUND(\n",
    "        CAST(SUM(logged_consecutive) AS FLOAT) / \n",
    "        CAST(COUNT(*) AS FLOAT), \n",
    "        2\n",
    "    ) as fraction\n",
    "FROM ConsecutiveLogins;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#leetcode-sql-problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
